As we may teach 1 Designing pedagogical approaches to artificial intelligence Code / space - Intelligence in algorithms - Right, wrong: who decides? - Machine teaching - Designing datasets - cognitive biases - Ethics in A.i. Leading question: Who designs algorithms and who designs the resources from which algorithms learn? Central focus: A need for a figure that designs the datasets used to train Artificial Intelligences, as they appear more and more often filled with stereotypes from our society. At the same time there will also be the need for a figure capable of thinking how to deal with these biases: should they be removed or not? What kind of vision of our world do we want to show to these machines? In here the figure of a designer lies, by thinking new ways to create and approach datasets, and by deciding how to engage with ethics-related problematics. This is not an abstract (yet) The disproportionate often irresponsive use of Machine learning Algorithms risks to emphasise stereotypes already present in data. Whenever data reflects biases of the broader society, the learning algorithm captures and learns from these stereotypes. Most of the work done by researchers and industries is focusing on the performance and optimization of learning algorithm, always from a learner’s perspective. Terms like “Algorithmic Bias” are showing up more often on international press when software used to predict future criminals are biased against blacks, or Google Translate assign a sex to gender neutral sentences. Even more examples could be mentioned, from Extremist ads on Youtube to links between apps for gay men and sex offenders. Machines just learn from what we show them. The misconfusing definitions of Artificial Intelligence or Machine learning should be instead replaced by the term Machine Teaching, as it is in the teacher that lies the responsibility of deciding what to transmit, what task and what to study. Placing the responsibility on the teacher helps us to realise how tricky this process is. We shouldn’t just focus on fixing the mistakes in these machines, but we have to be more careful on deciding which examples we give them as inputs and therefore on what we teach them. Artificial Intelligence is now in every object, from phones to lights, from maps to lockers. Critiques on Artificial intelligence are often focused on speculation, mainly with a sci-fi dystopian point of view. Researchers are constantly improving the performance of the employed algorithms, testing their “intelligence” in many different contexts. Examples often depicts Computers defeating the world champion of Go, but a game that has clear paths to victory or defeat shouldn’t be the case study we rely upon on defining intelligence. Intelligence is more than a sophisticated form of pattern recognition, intelligence is also social and tactile as researches in cognitive science suggests: bodily interactions are crucial to how we grasp more abstract mental concepts. A.I should be therefore studied, tested and embodied in different environments. These algorithms are often seen and described, even by tech companies, as black boxes. However, these programs are not only creating new functionalities, but way too often also emphasizing distinctions between classes, gender and nationality as there is still not a proper figure that design the resources from which A.I. are trained. Although researcher too often choose quantity over quality in the dataset they’re training, focusing on performance and ignoring possible biases. We should begin to approach this topic as a new kind of social science, focused on explaining the social, economic, and spatial contours of software and data. As seen from the recent scandals of Cambridge Analytica and Facebook, the data and the decision made by algorithms trained upon these data should be more of a public matter, so who is in charge to decide what these machines should learn? The focus of a designer should expand towards designing the information we want to communicate to these algorithms and that thinks about what kind of “vision of our world” we want to transmit to these machines. The missing figure i’m referring might seem much closer to a pedagog than a designer, but aside the fact that pedagog never finds himself dealing with datasets and A.I., while is starting to get more common for a designer, this last not only has to “educate” an Artificial Intelligence, but he/she has to design new ways to collectively teach Artificial Intelligences to avoid leaving the control of these machines to few programmers and companies. > in here the figure of a designer might design new ways to approach datasets, perhaps creating “democratic” and open source systems. As a Designer that sets up an Exhibition, the designer that creates a dataset should follow the same design process. designer -> Exposition -> Human designer -> dataset-> AI An exhibition designer thinks about what to display to the public and how to show it, the “dataset designer” instead has to take the same decisions to facilitate A.I’s learning. The designer takes upon her/himself the responsibility to decide where to gather informations from, how to order them and consequently how to deal with biases. It is crucial to dwell on this final point as it rises a discussion on who should fix biases when they appear and what to do when machines go off the rails. When in 2016 Twitter released “Tay”, the bot started to learn from interactions with users and begane to post offensive tweets on its account. In less than 16 hours the creators shutted it off. How could one judge A.i. on ethical grounds? A way to prevent that only one person is able to judge, as it was mentioned before, is to design open source systems to collectively create datasets, with more of a “democratic” structure. This would lead to a system that learns from the decision of the majority of people interacting with it and possible issues would be discussed and voted. This kind of approach would also allow the pure creation of new data, instead of continuing on “crawling” what’s available on the web, a process that is becoming problematic as it rarely produce any usable content. Chip Huyen, a teacher of the Computer Science Department at Stanford, is embarking a project to build the first realistic dialog dataset by asking people to send to her private chat logs, as the main used online are from Twitter posts, Hollywood movies or Reddit comments. Building a chatbot with those inputs would only lead to random conversation such as: “ Q: what do you like to do in your free time? A: what about the gun? ”. “It is easier to remove biases from algorithms than from people, and so ultimately AI has the potential to create a future where important decisions, such as hiring, diagnoses and legal judgements, are made in a more fair way.” Researchers are starting to remove biases whenever they start to notice them, and on the web is already possible to find “debiased” version of different datasets, but this approach is neither “right” and it’s not preventing future algorithms to be biased. This is why we should begin to approach A.i. following a pedagogical model: we wouldn’t teach a child to be racist and the day after try to remove the concept of racism. We should decide what and how to teach beforehand. Examples that on a different level are close to this topic ( and add another complex layer to it) are the questions if we should remove Confederate heritage in the U.S and fascist architecture in Italy. Even if these statues and buildings might fuel rightist nostalgia, the decision to remove a part of our history shouldn’t be taken lightly. We might even ask ourselves if we perhaps should even avoid to fix biases in these machines, as they reflect our society, but instead acknowledge that. We shouldn’t see this as problems to solve but as possibilities, what could they teach us? And whose responsibility is to decide what’s right and what’s wrong? Do we want to communicate a different vision of our reality by removing stereotypes or do we want to transmit an exact, but biased, representation of our world? > The designer is the figure capable of raising questions and spreading awareness, by creating interfaces that visualise what algorithms are learning. The designer could also create system to ... “To be starting points for conversation, designers must first acknowledge that recommendation systems (both those that are run by humans and those relying upon algorithms) have the power to suggest and constrain expression.” to phrase: If I copy my brain/body, does it have a right to vote, or is it redundant? Consider that the copies begin to diverge immediately or the copy could be intentionally different. In addition to passing the maturity/sanity/humanity test, perhaps the copy needs to pass a reverse-Turing test (a Church-Turing test?). Rather than demonstrating behavior indistinguishable from a human, the goal would be to show behavior distinct from human individuals. ( George Church Professor, Harvard University; Director, Personal Genome Project; Co-author (with Ed Regis), Regenesis ) Artificial intelligence is not the product of an alien invasion. It is an artifact of a particular human culture, and reflects the values of that culture. Edsger Dijkstra (The question of whether machines can think is about as relevant as the question of whether submarines can swim ) Cognitive scientists have discovered a second set of brain circuits dedicated to the representation of other minds—what other people think, know or believe. Unless we suffer from a disease called autism, all of us constantly pay attention to others and adapt our behavior to their state of knowledge—or rather to what we think that they know. Such "theory-of-mind" is the second crucial ingredient that current software lacks: a capacity to attend to its user. Future software should incorporate a model of its user. Can she properly see my display, or do I need to enlarge the characters? Do I have any evidence that my message was understood and heeded? Even a minimal simulation of the user would immediately give a strong impression that the machine is "thinking". This is because having a theory-of-mind is required to achieve relevance (a concept first modeled by cognitive scientist Dan Sperber). Unlike present-day computers, humans do not say utterly irrelevant things, because they pay attention to how their interlocutors will be affected by what they say. The navigator software that tells you "at the next roundabout, take the second exit" sounds stupid because it doesn't know that "go straight" would be a much more compact and relevant message. in laboratory of Professor Martin Fischer at the University of Potsdam, extremely interesting research is being done on the connection of the body and mathematical reasoning. Stephen Levinson's group at the Max Planck Institute for Psycholinguistics in Nijmegen has shown how culture can affect navigational abilities—a vital cognition function of most species. In my own research, I am looking at the influence of culture on the formation of what I refer to as "dark matter of the mind," a set of knowledges, orientations, biases, and patterns of thought that affect our cognition profoundly and pervasively. to add: the problem with current datasets (numbers and testings - Mnist - CelebA) The concept of New in Ai (but not really) - connected to this last the following Dcgan images - how to use them usefully?